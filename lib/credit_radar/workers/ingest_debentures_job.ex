defmodule CreditRadar.Workers.IngestDebenturesJob do
  @moduledoc """
  Oban worker for processing debenture XLS file uploads.

  This job reads the XLSX file and enqueues individual ProcessDebentureRowJob
  for each row. This approach avoids OOM by distributing work across many small jobs.
  """
  use Oban.Worker, queue: :debentures, max_attempts: 3

  alias CreditRadar.Ingestions
  alias CreditRadar.Workers.ProcessDebentureRowJob
  alias CreditRadar.Repo
  alias Decimal

  require Logger

  @impl Oban.Worker
  def perform(%Oban.Job{args: %{"execution_id" => execution_id, "file_path" => file_path}}) do
    Logger.info("游릭 Starting IngestDebenturesJob for execution ##{execution_id}")
    Logger.info("游릭 File path: #{file_path}")

    # Load the execution record
    execution = Repo.get!(Ingestions.Execution, execution_id)

    # Mark execution as running
    {:ok, execution} =
      execution
      |> Ingestions.execution_update_changeset(%{"status" => "running"})
      |> Repo.update()

    Logger.info("游릭 Execution ##{execution_id} marked as running")

    # Parse file and enqueue row jobs
    try do
      case parse_and_enqueue_rows(file_path, execution_id) do
        {:ok, row_count} ->
          Logger.info("游릭 IngestDebenturesJob completed: enqueued #{row_count} row jobs for execution ##{execution_id}")

          # Mark execution as completed (row jobs will process in background)
          {:ok, _execution} =
            execution
            |> Ingestions.execution_update_changeset(%{
              "status" => "completed",
              "progress" => 100,
              "finished_at" => DateTime.utc_now()
            })
            |> Repo.update()

          :ok

        {:error, reason} = error ->
          Logger.error("游댮 IngestDebenturesJob failed for execution ##{execution_id}")
          Logger.error("游댮 Error: #{inspect(reason)}")

          # Mark execution as failed
          {:ok, _execution} =
            execution
            |> Ingestions.execution_update_changeset(%{
              "status" => "failed",
              "finished_at" => DateTime.utc_now()
            })
            |> Repo.update()

          error
      end
    after
      # Clean up the uploaded file
      Logger.info("游릭 Cleaning up file: #{file_path}")
      File.rm(file_path)
    end
  end

  @doc """
  Parses XLSX file and enqueues a job for each row.

  Reads file incrementally to avoid loading entire file in memory.
  """
  defp parse_and_enqueue_rows(file_path, execution_id) do
    unless File.exists?(file_path) do
      {:error, :file_not_found}
    else
      try do
        # Just read the file and enqueue jobs - no inline string extraction here
        Logger.info("Reading XLSX file...")
        {:ok, pid} = Xlsxir.multi_extract(file_path, 0)
        rows = Xlsxir.get_list(pid)

        Logger.info("Found #{length(rows)} total rows (including header)")

        # Skip header and enqueue job for each row (with row data for later processing)
        row_count =
          rows
          |> Enum.drop(1)
          |> Enum.with_index(2)
          |> Enum.reduce(0, fn {row, row_index}, count ->
            # Just enqueue with row index and numeric data - no inline strings yet
            if Enum.all?(row, &is_nil/1) do
              count
            else
              # Enqueue job with minimal data
              %{
                row_index: row_index,
                row_data: row,
                file_path: file_path,
                execution_id: execution_id
              }
              |> ProcessDebentureRowJob.new()
              |> Oban.insert!()

              count + 1
            end
          end)

        Xlsxir.close(pid)

        Logger.info("Enqueued #{row_count} row processing jobs")

        {:ok, row_count}
      rescue
        error ->
          Logger.error("Failed to parse XLSX file: #{inspect(error)}")
          {:error, {:parse_error, error}}
      end
    end
  end


  @impl Oban.Worker
  def timeout(_job), do: :timer.minutes(30)
end
